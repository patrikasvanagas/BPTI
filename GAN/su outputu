{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"su outputu","provenance":[{"file_id":"1j9Rmnx107HGfBUEFYJjXPi1ZXO8ARmo0","timestamp":1626162244747},{"file_id":"1tyJHjd1C8Ss9fOEVL834XqpsZ2RlZ7SI","timestamp":1626077612061}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_hhsMTBS4PT","executionInfo":{"status":"ok","timestamp":1628689577988,"user_tz":480,"elapsed":34660,"user":{"displayName":"Patrikas Vanagas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_CUJcyzs73rqoGkaW2oHdMRWPMUzuDMyQbjouLDI=s64","userId":"11468990506011328105"}},"outputId":"1ebcc631-5062-49c4-f0fc-04004b7694c3"},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision.utils as vutils\n","from torchsummary import summary\n","from tqdm import tqdm\n","import itertools\n","import time\n","import os\n","from os.path import isfile, join\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from google.colab import drive\n","from PIL import Image\n","import cv2\n","from google.colab.patches import cv2_imshow\n","import random\n","import math\n","from collections import OrderedDict\n","from google.colab import drive\n","import sys\n","from tqdm import tqdm\n","\n","drive.mount('/content/drive')\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'); print(device)\n","\n","sys.path.append('/content/drive/MyDrive/BPTI_drone_archive/cycle_GAN_git/util')\n","import our_lib\n","\n","df = pd.read_json(r'/content/drive/MyDrive/BPTI_drone_archive/dataset/annotations.json')\n","dir_A = \"/content/drive/MyDrive/BPTI_drone_archive/dataset/trainA\"\n","dir_B = \"/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB\"\n","day = '22_07_'\n","topic = '131_is_vakar'\n","info = day + topic\n","img_size = 64\n","batch_size = 1\n","\n","\n","# our_lib.del_empty_pix(dir_A, dir_B, df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","cuda\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XiT_amTA8m5n"},"source":["class UnalignedDataset(Dataset):\n","\n","    def __init__(self, dir_A, dir_B, df, transform):\n","\n","        self.A_paths = [] \n","        self.B_paths = [] \n","        for files in os.listdir(dir_A):\n","          self.A_paths.append(os.path.join(dir_A, files)) \n","        for files in os.listdir(dir_B):\n","          self.B_paths.append(os.path.join(dir_B, files))\n","        self.A_paths.sort(key = lambda x: x[0:])\n","        self.A_paths.sort()\n","        self.B_paths.sort(key = lambda x: x[0:])\n","        self.B_paths.sort()\n","        self.A_size = len(self.A_paths)  \n","        self.B_size = len(self.B_paths)  \n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        A_path = self.A_paths[index % self.A_size] \n","        B_path = self.B_paths[index % self.B_size]\n","        A_name = str(A_path).split('/')[7].split(\"'\")[0]\n","        B_name = str(B_path).split('/')[7].split(\"'\")[0]\n","        B_bboxes = df[B_name]['bboxes'][0]\n","        A_img = Image.open(A_path).convert('RGB')\n","        A_xmin, A_ymin, A_xmax, A_ymax = our_lib.get_square_bboxes(df[A_name]['bboxes'][0])\n","        A_img = A_img.crop((A_xmin, A_ymin, A_xmax, A_ymax))\n","        A = self.transform(A_img)\n","        B_img = Image.open(B_path).convert('RGB')\n","        B_xmin, B_ymin, B_xmax, B_ymax = our_lib.get_square_bboxes(df[B_name]['bboxes'][0])\n","        B_img = B_img.crop((B_xmin, B_ymin, B_xmax, B_ymax))\n","        B = self.transform(B_img)\n","\n","        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n","        \n","    def __len__(self):\n","        return max(self.A_size, self.B_size)\n","    def modify_commandline_options(parser, is_train):\n","        return parser\n","\n","\n","img_transforms =  transforms.Compose([\n","        transforms.Resize((img_size, img_size)),\n","        # transforms.RandomResizedCrop(img_size),\n","        # transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","    ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"id":"8o6XgBp_tNU2","executionInfo":{"status":"ok","timestamp":1628689834332,"user_tz":480,"elapsed":13683,"user":{"displayName":"Patrikas Vanagas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_CUJcyzs73rqoGkaW2oHdMRWPMUzuDMyQbjouLDI=s64","userId":"11468990506011328105"}},"outputId":"bf66990d-bef0-4397-b1a0-8d32cf68afa1"},"source":["datalord = UnalignedDataset(dir_A, dir_B, df, transform = img_transforms)\n","dataloader = DataLoader(datalord, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","tic = time.process_time()\n","real_batch = next(iter(dataloader))\n","# print(real_batch)\n","toc = time.process_time()\n","print(real_batch['A'].size())\n","print(str(toc-tic))\n","plt.figure(figsize=(6, 8), dpi=80)\n","plt.axis(\"off\")\n","plt.title(\"Training Images\")\n","\n","plt.imshow(np.transpose(vutils.make_grid(real_batch['A'].to(device), padding=2, normalize=True).cpu(),(1,2,0)))\n","\n","print(real_batch['A_paths'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 3, 64, 64])\n","0.048565403999999646\n","['/content/drive/MyDrive/BPTI_drone_archive/dataset/trainA/drone_from_car__1743.jpg']\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 480x640 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"H16BVY8CsAH_","cellView":"code"},"source":["#@title Parser\n","#@title \n","# !python base_options.py --dataroot \"/content/drive/MyDrive/BPTI_drone_archive/cycle_GAN_git/dataset\"\n","import argparse\n","\n","def initialize(parser):\n","    \"\"\"Define the common options that are used in both training and test.\"\"\"\n","    # basic parameters\n","    parser.add_argument('--dataroot', type=str, default='/content/drive/MyDrive/BPTI_drone_archive/cycle_GAN_git/dataset', help='path to images (should have subfolders trainA, trainB, valA, valB, etc)')\n","    parser.add_argument('--name', type=str, default='experiment_name', help='name of the experiment. It decides where to store samples and models')\n","    parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n","    parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n","    parser.add_argument('--isTrain', type=bool, default=True, help='Is model training')\n","    parser.add_argument('--lambda_identity', type=float, default=0.5, help='use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1')\n","    parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for cycle loss (A -> B -> A)')\n","    parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n","            \n","    # model parameters\n","    parser.add_argument('--model', type=str, default='cycle_gan', help='chooses which model to use. [cycle_gan | pix2pix | test | colorization]')\n","    parser.add_argument('--input_nc', type=int, default=3, help='# of input image channels: 3 for RGB and 1 for grayscale')\n","    parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels: 3 for RGB and 1 for grayscale')\n","    parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in the last conv layer')\n","    parser.add_argument('--ndf', type=int, default=64, help='# of discrim filters in the first conv layer')\n","    parser.add_argument('--netD', type=str, default='basic', help='specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator')\n","    parser.add_argument('--netG', type=str, default='resnet_9blocks', help='specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]')\n","    parser.add_argument('--n_layers_D', type=int, default=3, help='only used if netD==n_layers')\n","    parser.add_argument('--norm', type=str, default='instance', help='instance normalization or batch normalization [instance | batch | none]')\n","    parser.add_argument('--init_type', type=str, default='normal', help='network initialization [normal | xavier | kaiming | orthogonal]')\n","    parser.add_argument('--init_gain', type=float, default=0.02, help='scaling factor for normal, xavier and orthogonal.')\n","    parser.add_argument('--no_dropout', action='store_true', default = True, help='no dropout for the generator')\n","    # dataset parameters\n","    parser.add_argument('--dataset_mode', type=str, default='unaligned', help='chooses how datasets are loaded. [unaligned | aligned | single | colorization]')\n","    parser.add_argument('--direction', type=str, default='AtoB', help='AtoB or BtoA')\n","    parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n","    parser.add_argument('--num_threads', default=4, type=int, help='# threads for loading data')\n","    parser.add_argument('--batch_size', type=int, default=1, help='input batch size')\n","    parser.add_argument('--load_size', type=int, default=img_size, help='scale images to this size')\n","    parser.add_argument('--crop_size', type=int, default=img_size, help='then crop to this size')\n","    parser.add_argument('--max_dataset_size', type=int, default=float(\"inf\"), help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n","    parser.add_argument('--preprocess', type=str, default='resize_and_crop', help='scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]')\n","    parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data augmentation')\n","    parser.add_argument('--display_winsize', type=int, default=256, help='display window size for both visdom and HTML')\n","    # additional parameters\n","    parser.add_argument('--epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n","    parser.add_argument('--load_iter', type=int, default='0', help='which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]')\n","    parser.add_argument('--verbose', action='store_true', help='if specified, print more debugging information')\n","    parser.add_argument('--suffix', default='', type=str, help='customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}')\n","  \n","    # visdom and HTML visualization parameters\n","    parser.add_argument('--display_freq', type=int, default= 500, help='frequency of showing training results on screen')\n","    parser.add_argument('--save_loss_freq', type=int, default= 300, help='frequency of showing training results on screen')\n","    parser.add_argument('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n","    parser.add_argument('--display_id', type=int, default=1, help='window id of the web display')\n","    parser.add_argument('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n","    parser.add_argument('--display_env', type=str, default='main', help='visdom display environment name (default is \"main\")')\n","    parser.add_argument('--display_port', type=int, default=8097, help='visdom port of the web display')\n","    parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n","    parser.add_argument('--print_freq', type=int, default=10, help='frequency of showing training results on console')\n","    parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n","    # network saving and loading parameters\n","    parser.add_argument('--save_latest_freq', type=int, default=30, help='frequency of saving the latest results') #buvo 30\n","    parser.add_argument('--save_epoch_freq', type=int, default=25, help='frequency of saving checkpoints at the end of epochs') #buvo 25\n","    parser.add_argument('--save_by_iter', action='store_true', help='whether saves model by iteration')\n","    parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n","    parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n","    parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n","    # training parameters\n","    parser.add_argument('--n_epochs', type=int, default=1, help='number of epochs with the initial learning rate')\n","    parser.add_argument('--n_epochs_decay', type=int, default=25, help='number of epochs to linearly decay learning rate to zero')\n","    parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n","    parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n","    parser.add_argument('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n","    parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n","    parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n","    parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n","\n","    parser.add_argument('-f')\n","\n","    initialized = True\n","    return parser\n","\n","initialized = False\n","if not initialized:  # check if it has been initialized\n","    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n","    parser = initialize(parser)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gMoRfepR0dm9"},"source":["os.chdir(\"/content/drive/MyDrive/BPTI_drone_archive/cycle_GAN_git/options\")\n","# from base_options import BaseOptions\n","from train_options import TrainOptions\n","# import __init__\n","from models import create_model\n","from models import networks\n","from models.base_model import BaseModel\n","from util.image_pool import ImagePool\n","\n","class CycleGANModel(BaseModel):\n","    \"\"\"\n","    This class implements the CycleGAN model, for learning image-to-image translation without paired data.\n","    The model training requires '--dataset_mode unaligned' dataset.\n","    By default, it uses a '--netG resnet_9blocks' ResNet generator,\n","    a '--netD basic' discriminator (PatchGAN introduced by pix2pix),\n","    and a least-square GANs objective ('--gan_mode lsgan').\n","    CycleGAN paper: https://arxiv.org/pdf/1703.10593.pdf\n","    \"\"\"\n","    @staticmethod\n","    def modify_commandline_options(parser, is_train=True):\n","        \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n","        Parameters:\n","            parser          -- original option parser\n","            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n","        Returns:\n","            the modified parser.\n","        For CycleGAN, in addition to GAN losses, we introduce lambda_A, lambda_B, and lambda_identity for the following losses.\n","        A (source domain), B (target domain).\n","        Generators: G_A: A -> B; G_B: B -> A.\n","        Discriminators: D_A: G_A(A) vs. B; D_B: G_B(B) vs. A.\n","        Forward cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)\n","        Backward cycle loss: lambda_B * ||G_A(G_B(B)) - B|| (Eqn. (2) in the paper)\n","        Identity loss (optional): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) (Sec 5.2 \"Photo generation from paintings\" in the paper)\n","        Dropout is not used in the original CycleGAN paper.\n","        \"\"\"\n","        parser.set_defaults(no_dropout=True)  # default CycleGAN did not use dropout\n","        if is_train:\n","            parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for cycle loss (A -> B -> A)')\n","            parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n","            parser.add_argument('--lambda_identity', type=float, default=0.5, help='use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1')\n","        return parser\n","\n","    def __init__(self, opt):\n","        \"\"\"Initialize the CycleGAN class.\n","        Parameters:\n","            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n","        \"\"\"\n","        BaseModel.__init__(self, opt)\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n","        print(self.device)\n","        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n","        self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n","        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n","        visual_names_A = ['real_A', 'fake_B', 'rec_A']\n","        visual_names_B = ['real_B', 'fake_A', 'rec_B']\n","        if self.isTrain and self.opt.lambda_identity > 0.0:  # if identity loss is used, we also visualize idt_B=G_A(B) ad idt_A=G_A(B)\n","            visual_names_A.append('idt_B')\n","            visual_names_B.append('idt_A')\n","\n","        self.visual_names = visual_names_A + visual_names_B  # combine visualizations for A and B\n","        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>.\n","        if self.isTrain:\n","            self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']\n","        else:  # during test time, only load Gs\n","            self.model_names = ['G_A', 'G_B']\n","\n","        # define networks (both Generators and discriminators)\n","        # The naming is different from those used in the paper.\n","        # Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n","        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n","                                        not opt.no_dropout, opt.init_type, opt.init_gain)\n","        \n","        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm,\n","                                        not opt.no_dropout, opt.init_type, opt.init_gain)\n","\n","        if self.isTrain:  # define discriminators\n","            self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n","                                            opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain)\n","            self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,\n","                                            opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain)\n","\n","        if self.isTrain:\n","            if opt.lambda_identity > 0.0:  # only works when input and output images have the same number of channels\n","                assert(opt.input_nc == opt.output_nc)\n","            self.fake_A_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n","            self.fake_B_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n","            # define loss functions\n","            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n","            self.criterionCycle = torch.nn.L1Loss()\n","            self.criterionIdt = torch.nn.L1Loss()\n","            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n","            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n","            self.optimizers.append(self.optimizer_G)\n","            self.optimizers.append(self.optimizer_D)\n","    def set_input(self, input):\n","        \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n","        Parameters:\n","            input (dict): include the data itself and its metadata information.\n","        The option 'direction' can be used to swap domain A and domain B.\n","        \"\"\"\n","        AtoB = self.opt.direction == 'AtoB'\n","        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n","        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n","        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n","\n","    def forward(self):\n","        \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n","        self.fake_B = self.netG_A(self.real_A)  # G_A(A)\n","        # print(self.fake_B.size())\n","        # print(self.fake_B[0].size())\n","\n","        # print(self.real_A.size())\n","        # print(self.real_A[0].size())\n","\n","        # cpu_fake = self.fake_B[0].detach().cpu()\n","        # plt.imshow(np.transpose(cpu_fake, (1,2,0)))\n","        # plt.imshow(np.transpose(self.fake_B[0].cpu(),(1,2,0)))\n","        self.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))\n","        self.fake_A = self.netG_B(self.real_B)  # G_B(B)\n","        self.rec_B = self.netG_A(self.fake_A)   # G_A(G_B(B))\n","\n","    def backward_D_basic(self, netD, real, fake):\n","        \"\"\"Calculate GAN loss for the discriminator\n","        Parameters:\n","            netD (network)      -- the discriminator D\n","            real (tensor array) -- real images\n","            fake (tensor array) -- images generated by a generator\n","        Return the discriminator loss.\n","        We also call loss_D.backward() to calculate the gradients.\n","        \"\"\"\n","        # Real\n","        pred_real = netD(real)\n","        loss_D_real = self.criterionGAN(pred_real, True)\n","        # Fake\n","        pred_fake = netD(fake.detach())\n","        loss_D_fake = self.criterionGAN(pred_fake, False)\n","        # Combined loss and calculate gradients\n","        loss_D = (loss_D_real + loss_D_fake) * 0.5\n","        loss_D.backward()\n","        return loss_D\n","\n","    def backward_D_A(self):\n","        \"\"\"Calculate GAN loss for discriminator D_A\"\"\"\n","        fake_B = self.fake_B_pool.query(self.fake_B)\n","        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n","\n","    def backward_D_B(self):\n","        \"\"\"Calculate GAN loss for discriminator D_B\"\"\"\n","        fake_A = self.fake_A_pool.query(self.fake_A)\n","        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n","\n","    def backward_G(self):\n","        \"\"\"Calculate the loss for generators G_A and G_B\"\"\"\n","        lambda_idt = self.opt.lambda_identity\n","        lambda_A = self.opt.lambda_A\n","        lambda_B = self.opt.lambda_B\n","        # Identity loss\n","        if lambda_idt > 0:\n","            # G_A should be identity if real_B is fed: ||G_A(B) - B||\n","            self.idt_A = self.netG_A(self.real_B)\n","            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n","            # G_B should be identity if real_A is fed: ||G_B(A) - A||\n","            self.idt_B = self.netG_B(self.real_A)\n","            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n","        else:\n","            self.loss_idt_A = 0\n","            self.loss_idt_B = 0\n","\n","        # GAN loss D_A(G_A(A))\n","        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n","        # GAN loss D_B(G_B(B))\n","        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n","        # Forward cycle loss || G_B(G_A(A)) - A||\n","        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n","        # Backward cycle loss || G_A(G_B(B)) - B||\n","        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n","        # combined loss and calculate gradients\n","        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\n","        self.loss_G.backward()\n","\n","    def optimize_parameters(self):\n","        \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n","        # forward\n","        self.forward()      # compute fake images and reconstruction images.\n","        # G_A and G_B\n","        self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n","        self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero\n","        self.backward_G()             # calculate gradients for G_A and G_B\n","        self.optimizer_G.step()       # update G_A and G_B's weights\n","        # D_A and D_B\n","        self.set_requires_grad([self.netD_A, self.netD_B], True)\n","        self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\n","        self.backward_D_A()      # calculate gradients for D_A\n","        self.backward_D_B()      # calculate graidents for D_B\n","        self.optimizer_D.step()  # update D_A and D_B's weights\n","\n","    def plotting_images(self, images, img_names):\n","        images = model.get_current_visuals()\n","        f, axarr = plt.subplots(1,len(image_names), figsize=(20,120))\n","        plot_nr = 0\n","        for name in img_names:    \n","          images[name][0] -= images[name][0].min()\n","          images[name][0] /= images[name][0].max()\n","          axarr[plot_nr].set_title(name)\n","          axarr[plot_nr].imshow(np.transpose(images[name][0].detach().cpu(),(1,2,0)))\n","          plot_nr +=1\n","        plt.show()\n","\n","    def saving_model(self, info,model_dict, model_name, number_of_epochs):\n","        save_dir = \"/content/drive/MyDrive/BPTI_drone_archive/saved_models\"\n","        save_filename = info + '_%s_net_%s.pth' % (number_of_epochs, model_name)\n","        save_path = os.path.join(save_dir, save_filename)\n","\n","        if torch.cuda.is_available():\n","          torch.save(model_dict, save_path)\n","\n","\n","    def load_networks(self, day_n_epoch):\n","      \"\"\"Load all the networks from the disk.\n","\n","      Parameters:\n","          epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n","      \"\"\"\n","      # os.chdir(\"/content/drive/MyDrive/BPTI_drone_archive/cycle_GAN_git/saved_models\")\n","      save_dir = \"/content/drive/MyDrive/BPTI_drone_archive/saved_models\"\n","      for name in self.model_names:\n","          if isinstance(name, str):\n","              load_filename = day_n_epoch +\"_net_\"+ name + \".pth\"\n","              print(load_filename)\n","              load_path = os.path.join(save_dir, load_filename)\n","              net = getattr(self, 'net' + name)\n","              if isinstance(net, torch.nn.DataParallel):\n","                  net = net.module\n","              print('loading the model from %s' % load_path)\n","              # if you are using PyTorch newer than 0.4 (e.g., built from\n","              # GitHub source), you can remove str() on self.device\n","              state_dict = torch.load(load_path, map_location=str(self.device))\n","              if hasattr(state_dict, '_metadata'):\n","                  del state_dict._metadata\n","\n","              # patch InstanceNorm checkpoints prior to 0.4\n","              # for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n","              #     self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n","              net.load_state_dict(state_dict)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AhCy1p3hrOEt","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1p8rYyadAf64_IIR4wIsB57EF6ewYoLUo"},"executionInfo":{"status":"error","timestamp":1628691868173,"user_tz":480,"elapsed":1003924,"user":{"displayName":"Patrikas Vanagas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_CUJcyzs73rqoGkaW2oHdMRWPMUzuDMyQbjouLDI=s64","userId":"11468990506011328105"}},"outputId":"4aaa568a-9dcc-46e5-df69-75dcf986d95b"},"source":["\"\"\"General-purpose training script for image-to-image translation.\n","This script works for various models (with option '--model': e.g., pix2pix, cyclegan, colorization) and\n","different datasets (with option '--dataset_mode': e.g., aligned, unaligned, single, colorization).\n","You need to specify the dataset ('--dataroot'), experiment name ('--name'), and model ('--model').\n","It first creates model, dataset, and visualizer given the option.\n","It then does standard network training. During the training, it also visualize/save the images, print/save the loss plot, and save models.\n","The script supports continue/resume training. Use '--continue_train' to resume your previous training.\n","Example:\n","    Train a CycleGAN model:\n","        python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan\n","    Train a pix2pix model:\n","        python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n","See options/base_options.py and options/train_options.py for more training options.\n","See training and test tips at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md\n","See frequently asked questions at: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/qa.md\n","\"\"\"\n","\n","!pip install dominate\n","import dominate\n","!pip install visdom\n","import visdom\n","from util.visualizer import Visualizer\n","\n","os.chdir(\"/content/drive/MyDrive/BPTI_drone_archive/cycle_GAN_git\")\n","from data.unaligned_dataset import UnalignedDataset\n","\n","opt = parser.parse_args()   # get training options\n","\n","loss_list = []\n","\n","dataset = dataloader  # create a dataset given opt.dataset_mode and other options\n","\n","dataset_size = len(dataset)    # get the number of images in the dataset.\n","print('The number of training images = %d' % dataset_size)\n","\n","\n","\n","# # 2 uzkomentuoti kai uzkrovei jau treniruota\n","# model = CycleGANModel(opt)      # create a model given opt.model and other options\n","# model.setup(opt)               # regular setup: load and print networks; create schedulers\n","visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots\n","total_iters = 0                # the total number of training iterations\n","\n","for epoch in (range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1)):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n","    epoch_start_time = time.time()  # timer for entire epoch\n","    iter_data_time = time.time()    # timer for data loading per iteration\n","    epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n","    # visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n","    model.update_learning_rate()    # update learning rates in the beginning of every epoch.\n","    for i, data in (enumerate(dataset)):  # inner loop within one epoch\n","        iter_start_time = time.time()  # timer for computation per iteration\n","        if total_iters % opt.print_freq == 0:\n","            t_data = iter_start_time - iter_data_time\n","\n","        total_iters += opt.batch_size\n","        epoch_iter += opt.batch_size\n","        # svarbu loadinant\n","        model.set_input(data)         # unpack data from dataset and apply preprocessing\n","        model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n","        # print(total_iters)\n","        if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file\n","\n","            image_names = ['real_A', 'fake_B', 'rec_A','real_B', 'fake_A', 'rec_B']\n","            model.plotting_images(model.get_current_visuals(), image_names)\n","\n","        if total_iters % opt.save_loss_freq == 0:    # print training losses and save logging information to the disk\n","            losses = model.get_current_losses()\n","            loss_list.append(losses)\n","            t_comp = (time.time() - iter_start_time) / opt.batch_size\n","            visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n","            if opt.display_id > 0:\n","                visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n","        iter_data_time = time.time()\n","\n","    if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n","        print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n","        model.saving_model(info,model.netG_A.state_dict(), 'G_A', epoch + 131)\n","        model.saving_model(info,model.netG_B.state_dict(), 'G_B', epoch + 131)\n","        model.saving_model(info,model.netD_A.state_dict(), 'D_A', epoch + 131)\n","        model.saving_model(info,model.netD_B.state_dict(), 'D_B', epoch + 131)\n","\n","    print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":162},"id":"jHtv9l3m2Xkk","executionInfo":{"status":"error","timestamp":1627905642159,"user_tz":480,"elapsed":407,"user":{"displayName":"Patrikas Vanagas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_CUJcyzs73rqoGkaW2oHdMRWPMUzuDMyQbjouLDI=s64","userId":"11468990506011328105"}},"outputId":"d127c7ee-835a-43e7-b035-075d01428bfc"},"source":["our_lib.loss_plot(loss_list)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-b5a072432a28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'loss_list' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1fxy4HbInXmE","executionInfo":{"status":"ok","timestamp":1628690839741,"user_tz":480,"elapsed":6229,"user":{"displayName":"Patrikas Vanagas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_CUJcyzs73rqoGkaW2oHdMRWPMUzuDMyQbjouLDI=s64","userId":"11468990506011328105"}},"outputId":"24d5376b-cce0-4508-d87d-0b14ce7ecde6"},"source":["model.load_networks(\"21_07_naktinis_131\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["21_07_naktinis_131_net_G_A.pth\n","loading the model from /content/drive/MyDrive/BPTI_drone_archive/saved_models/21_07_naktinis_131_net_G_A.pth\n","21_07_naktinis_131_net_G_B.pth\n","loading the model from /content/drive/MyDrive/BPTI_drone_archive/saved_models/21_07_naktinis_131_net_G_B.pth\n","21_07_naktinis_131_net_D_A.pth\n","loading the model from /content/drive/MyDrive/BPTI_drone_archive/saved_models/21_07_naktinis_131_net_D_A.pth\n","21_07_naktinis_131_net_D_B.pth\n","loading the model from /content/drive/MyDrive/BPTI_drone_archive/saved_models/21_07_naktinis_131_net_D_B.pth\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9uA9_tae13sY"},"source":["# with torch.no_grad():\n","fake_A = []\n","fake_B = []\n","\n","\n","\n","for i, data in enumerate(dataset):\n","  model.test()\n","  model.set_input(data)  \n","  image_names = model.get_current_visuals()\n","\n","  B = image_names['fake_B'][0]\n","  fake_B.append((image_names['fake_B'][0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKi32la0CnyZ","cellView":"form"},"source":["#@title Default plotter\n","for i, data in enumerate(dataset):\n","  model.test()\n","  model.set_input(data)  \n","  image_names = ['real_A', 'fake_B', 'rec_A','real_B', 'fake_A', 'rec_B']\n","  model.plotting_images(model.get_current_visuals(), image_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"UWL5UBWBeLJd"},"source":["#@title sorted dataset\n","#create fake dataset\n","\n","img_size = 64\n","to_tensor = transforms.ToTensor()\n","tensor_resize = transforms.Resize((img_size, img_size))\n","\n","sorted_input = OrderedDict({'A' : torch.empty, 'A_paths' : [], 'B'  : torch.empty, 'B_paths' : []})\n","\n"," \n","\n","files_A = os.listdir(dir_A)\n","files_B = os.listdir(dir_B)\n","\n","A_paths = os.listdir(dir_A)\n","B_paths = os.listdir(dir_B)\n","\n","for i in range(len(files_A)):\n","  A_paths[i] = dir_A + '/' + files_A[i]\n","  \n","A_paths.sort(key = lambda x: x[0:])\n","A_paths.sort()\n","\n","img_list = []\n","for i in range(len(files_A)):\n","  A_name = str(A_paths[i]).split('/')[8].split(\"'\")[0]\n","  A_img = Image.open(A_paths[i]).convert('RGB')\n","  # A_xmin, A_ymin, A_xmax, A_ymax = square_bboxes(df[A_name]['bboxes'][0])\n","  # A_img = A_img.crop((A_xmin, A_ymin, A_xmax, A_ymax))\n","  A_img = to_tensor(A_img)\n","  A_img = tensor_resize(A_img)\n","  A_img = torch.unsqueeze(A_img, 0)\n","  sorted_input['A'][i] = A_img\n","\n","\n","\n","  \n","for i in range(len(files_B)):\n","  B_paths[i] = dir_B + '/' + files_B[i]\n","  \n","B_paths.sort(key = lambda x: x[0:])\n","B_paths.sort()\n","\n","for i in range(len(files_B)):\n","  B_name = str(B_paths[i]).split('/')[8].split(\"'\")[0]\n","  B_img = Image.open(B_paths[i]).convert('RGB')\n","  # B_xmin, B_ymin, B_xmax, B_ymax = square_bboxes(df[B_name]['bboxes'][0])\n","  # B_img = B_img.crop((B_xmin, B_ymin, B_xmax, B_ymax))\n","  B_img = to_tensor(B_img)\n","  B_img = tensor_resize(B_img)\n","  B_img = torch.unsqueeze(B_img, 0)\n","  img_list.append(B_img)\n","\n","  # sorted_input['B'][0].append(B_img)\n","\n","sorted_input['A_paths'] = A_paths\n","sorted_input['B_paths'] = B_paths\n","\n","\n","# with torch.no_grad:\n","for i in range(len(img_list)):\n","  # model.test()\n","  model.netG_A.eval()\n","  img_list[i].detach()\n","  img = img_list[i].to(device)\n","  dummy = model.netG_A(img_list[i])\n","  print(dummy)\n","  # model.set_input(img_list[i])  \n","  # \n","  # image_names = ['real_A', 'fake_B', 'rec_A','real_B', 'fake_A', 'rec_B']\n","  # model.plotting_images(model.get_current_visuals(), image_names)\n","\n","  sorted_input['A'] = to_tensor(sorted_input['A'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JM-IgntUE6bD","executionInfo":{"status":"ok","timestamp":1627037306126,"user_tz":480,"elapsed":5304,"user":{"displayName":"Patrikas Vanagas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi_CUJcyzs73rqoGkaW2oHdMRWPMUzuDMyQbjouLDI=s64","userId":"11468990506011328105"}},"outputId":"fe2f2d4a-3988-4544-89ee-da737312043c"},"source":["path_in= \"/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/\"\n","path_g_out = \"/content/drive/MyDrive/BPTI_drone_archive/dataset/1_output_vids/generated.mp4\"\n","path_o_out = \"/content/drive/MyDrive/BPTI_drone_archive/dataset/1_output_vids/original.mp4\"\n","fps = 24\n","\n","gen_vid_out(path_in, path_g_out, df, fake_B, fps)\n","our_lib.og_vid_out(path_in, path_o_out, fps)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__1948.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__1998.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__2048.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__2098.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__2370.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__2431.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__3549.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__3599.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__3813.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__6645.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__6695.jpg\n","/content/drive/MyDrive/BPTI_drone_archive/dataset/trainB/drone_from_car__6745.jpg\n","Generated video up\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0QNCAIcfk39L"},"source":[""],"execution_count":null,"outputs":[]}]}