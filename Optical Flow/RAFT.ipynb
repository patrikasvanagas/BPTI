{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0519b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%capture\n",
    "# %matplotlib inline\n",
    "# !pip install av\n",
    "# !pip install torchvision==0.12.0\n",
    "# !pip install pytube\n",
    "\n",
    "# # RESTART RUNTIME AFTER INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe05c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  <https://arxiv.org/abs/2003.12039>\n",
    "\n",
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "\n",
    "from torchvision.utils import flow_to_image\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.io import read_video\n",
    "from pytube import YouTube\n",
    "from tqdm.notebook import tqdm\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import ffmpeg\n",
    "import winsound\n",
    "# from google.colab import drive\n",
    "# from google.colab import files\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# shutil.unpack_archive(\"drive/MyDrive//Scientific ISR/PESMOD.zip\", \"/content/drive/My Drive/Scientific ISR/PESMOD\")\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = raft_large(pretrained=True, progress=False).to(DEVICE)\n",
    "model = model.eval()\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d18680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(imgs, figsize=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(\n",
    "        nrows=num_rows, ncols=num_cols, squeeze=False, figsize=figsize\n",
    "    )\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            img = F.to_pil_image(img.to(\"cpu\"))\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def preprocess(batch):\n",
    "    # image dimension must be divisible by 8\n",
    "    transforms = T.Compose(\n",
    "        [\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize(mean=0.5, std=0.5),  # map [0, 1] into [-1, 1]\n",
    "            T.Resize(size=(HEIGHT, WIDTH)),\n",
    "        ]\n",
    "    )\n",
    "    batch = transforms(batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def download_youtube(videourl, path):\n",
    "    yt = YouTube(videourl)\n",
    "    # title = yt.streams[0].title\n",
    "    yt = (\n",
    "        yt.streams.filter(progressive=True, file_extension=\"mp4\")\n",
    "        .order_by(\"resolution\")\n",
    "        .desc()\n",
    "        .first()\n",
    "    )\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    yt.download(path)\n",
    "    # return title\n",
    "\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", key)]\n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "\n",
    "def vid2frame(path_in):\n",
    "    old_wd = os.getcwd()\n",
    "    vidcap = cv2.VideoCapture(path_in)\n",
    "    # success, image = vidcap.read()\n",
    "    image = vidcap.read()[1]\n",
    "    frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    path_out = path_in.replace(\".mp4\", \"_frames/\")\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    for o in os.listdir(path_out):\n",
    "        os.remove(os.path.join(path_out, o))\n",
    "    os.chdir(path_out)\n",
    "    print(\"Converting \" + path_in + \" to frames:\")\n",
    "    for m in tqdm(range(frames)):\n",
    "        cv2.imwrite(\"%d.jpg\" % m, image)\n",
    "        # success, image = vidcap.read()\n",
    "        image = vidcap.read()[1]\n",
    "    vidcap.release()\n",
    "    os.chdir(old_wd)\n",
    "\n",
    "\n",
    "def frame2vid(path_in, path_out, fps):\n",
    "    old_wd = os.getcwd()\n",
    "    # path_out = path_in.strip(\"/\") + \".mp4\"\n",
    "    frame_array = []\n",
    "    files = [f for f in os.listdir(path_in) if os.path.isfile(os.path.join(path_in, f))]\n",
    "    files = sorted_alphanumeric(files)\n",
    "    print(\"Reading \" + path_in + \" for conversion to video:\")\n",
    "    for m in tqdm(range(len(files))):\n",
    "        filename = path_in + files[m]\n",
    "        img = cv2.imread(filename)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width, height)\n",
    "        frame_array.append(img)\n",
    "    out = cv2.VideoWriter(path_out, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, size)\n",
    "    print(\"Converting \" + path_in + \" to video:\")\n",
    "    for m in tqdm(range(len(frame_array))):\n",
    "        out.write(frame_array[m])\n",
    "    out.release()\n",
    "    os.chdir(old_wd)\n",
    "\n",
    "\n",
    "def concatenate_frames2video(\n",
    "    path_in_of, path_video, path_out, start_time, end_time, fps\n",
    "):\n",
    "    old_wd = os.getcwd()\n",
    "    frame_array = []\n",
    "    files = [\n",
    "        f for f in os.listdir(path_in_of) if os.path.isfile(os.path.join(path_in_of, f))\n",
    "    ]\n",
    "    files = sorted_alphanumeric(files)\n",
    "    original_frames, _, _ = torchvision.io.read_video(\n",
    "        path_video, start_pts=start_time, end_pts=end_time, pts_unit=\"sec\"\n",
    "    )\n",
    "    original_frames = original_frames.permute(\n",
    "        0, 3, 1, 2\n",
    "    )  # (N, H, W, C) -> (N, C, H, W)\n",
    "    num_frames = len(original_frames)\n",
    "    indices = np.arange(0, num_frames)\n",
    "    print(\"Reading \" + path_in_of + \" for conversion to video:\")\n",
    "    for m in tqdm(range(len(files))):\n",
    "        filename = path_in_of + files[m]\n",
    "        right_img = cv2.imread(filename)\n",
    "        left_img = cv2.resize(\n",
    "            np.array(F.to_pil_image(original_frames[m - 1]))[:, :, ::-1],\n",
    "            (WIDTH, HEIGHT),\n",
    "        )\n",
    "        img = np.concatenate([left_img, right_img], axis=1)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width, height)\n",
    "        frame_array.append(img)\n",
    "    out = cv2.VideoWriter(path_out, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, size)\n",
    "    print(\"Converting \" + path_in_of + \" to video:\")\n",
    "    for m in tqdm(range(len(frame_array))):\n",
    "        out.write(frame_array[m])\n",
    "    out.release()\n",
    "    os.chdir(old_wd)\n",
    "\n",
    "\n",
    "def cosine_distance_background_reduction(input_flow, threshold=1):\n",
    "    input_flow_mean = torch.stack(\n",
    "        (\n",
    "            torch.ones(input_flow[0].size(), device=DEVICE) * input_flow[0].mean(),\n",
    "            torch.ones(input_flow[0].size(), device=DEVICE) * input_flow[1].mean(),\n",
    "        )\n",
    "    )\n",
    "    calculate_cosine_similarity = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "#     uzdet thresholda panasumui thresholda ant sitoj eilutej input_flow_mean\n",
    "    cosine_similarity = calculate_cosine_similarity(input_flow, input_flow_mean)\n",
    "    mean_cosine_similarity = cosine_similarity.mean()\n",
    "    indices_to_keep = cosine_similarity < mean_cosine_similarity * threshold\n",
    "    output_flow = input_flow * indices_to_keep\n",
    "    return output_flow\n",
    "\n",
    "def get_grid(shape):\n",
    "    x = torch.arange(1, shape[0] + 1, 1.0)\n",
    "    y = torch.arange(1, shape[1] + 1, 1.0)\n",
    "\n",
    "    return torch.meshgrid(x, y, indexing=\"ij\")\n",
    "\n",
    "# Operates on GPU\n",
    "def fit_2d_surface(x_grid, y_grid, z):\n",
    "    features = torch.stack([\n",
    "        torch.ones_like(x_grid), x_grid, y_grid, x_grid * x_grid, x_grid * y_grid, y_grid * y_grid\n",
    "    ]).reshape((6, -1)).T.cuda()\n",
    "\n",
    "    solution = torch.linalg.lstsq(features, z.flatten())\n",
    "\n",
    "    z_fitted = (features @ solution.solution).reshape(z.shape)\n",
    "\n",
    "    z_residuals = z - z_fitted\n",
    "\n",
    "    return solution, z_fitted, z_residuals\n",
    "\n",
    "# Flow must be cuda of shape (2, H, W)\n",
    "def get_detection_mask(flow, threshold):\n",
    "    flow_magnitudes = torch.linalg.norm(flow, axis=0)\n",
    "    x_grid, y_grid = get_grid(shape=flow_magnitudes.shape)\n",
    "\n",
    "    solution, flow_magnitudes_fitted, flow_magnitude_residuals = fit_2d_surface(\n",
    "        x_grid, y_grid, flow_magnitudes\n",
    "    )\n",
    "\n",
    "    return torch.abs(flow_magnitude_residuals) > threshold\n",
    "\n",
    "def get_detection(flow, threshold=1.25):\n",
    "    detection_mask = get_detection_mask(flow, threshold=threshold)\n",
    "\n",
    "    return detection_mask * flow\n",
    "\n",
    "def ffmpeg_xstack(input_path, output_path, *, fps):\n",
    "    inputs = [\n",
    "\n",
    "        ffmpeg.input(subdir / '%d.jpg', framerate=fps)\n",
    "        for subdir in sorted(input_path.iterdir())\n",
    "        if subdir.is_dir()\n",
    "    ]\n",
    "\n",
    "    ffmpeg \\\n",
    "        .filter(inputs, 'xstack', inputs=4, layout='0_0|w0_0|0_h0|w0_h0') \\\n",
    "        .output(str(output_path)) \\\n",
    "        .run(overwrite_output=True)\n",
    "\n",
    "    return Video(output_path, width=1280, height=720)\n",
    "\n",
    "\n",
    "def ffmpeg_hstack(video_path, frames_path, output_path, *, fps):\n",
    "    inputs = [\n",
    "        ffmpeg.input(video_path, ss=FROM, to=TO),\n",
    "        ffmpeg.input(frames_path / '%d.jpg', framerate=fps)\n",
    "    ]\n",
    "\n",
    "    ffmpeg \\\n",
    "        .filter(inputs, 'hstack') \\\n",
    "        .output(str(output_path)) \\\n",
    "        .run(overwrite_output=True)\n",
    "\n",
    "    return Video(output_path, width=1280, height=720)\n",
    "\n",
    "\n",
    "\n",
    "# def flow_to_image(flow: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "#     \"\"\"\n",
    "#     Converts a flow to an RGB image.\n",
    "\n",
    "#     Args:\n",
    "#         flow (Tensor): Flow of shape (N, 2, H, W) or (2, H, W) and dtype torch.float.\n",
    "\n",
    "#     Returns:\n",
    "#         img (Tensor): Image Tensor of dtype uint8 where each color corresponds\n",
    "#             to a given flow direction. Shape is (N, 3, H, W) or (3, H, W) depending on the input.\n",
    "#     \"\"\"\n",
    "\n",
    "#     if flow.dtype != torch.float:\n",
    "#         raise ValueError(f\"Flow should be of dtype torch.float, got {flow.dtype}.\")\n",
    "#     orig_shape = flow.shape\n",
    "#     if flow.ndim == 3:\n",
    "#         flow = flow[None]  # Add batch dim\n",
    "#     if flow.ndim != 4 or flow.shape[1] != 2:\n",
    "#         raise ValueError(\n",
    "#             f\"Input flow should have shape (2, H, W) or (N, 2, H, W), got {orig_shape}.\"\n",
    "#         )\n",
    "#     max_norm = torch.sum(flow**2, dim=1).sqrt().max()\n",
    "#     epsilon = torch.finfo((flow).dtype).eps\n",
    "#     normalized_flow = flow / (max_norm + epsilon)\n",
    "#     img = _normalized_flow_to_image(normalized_flow)\n",
    "\n",
    "#     if len(orig_shape) == 3:\n",
    "#         img = img[0]  # Remove batch dim\n",
    "#     return img\n",
    "\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def _normalized_flow_to_image(normalized_flow: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "#     \"\"\"\n",
    "#     Converts a batch of normalized flow to an RGB image.\n",
    "\n",
    "#     Args:\n",
    "#         normalized_flow (torch.Tensor): Normalized flow tensor of shape (N, 2, H, W)\n",
    "#     Returns:\n",
    "#        img (Tensor(N, 3, H, W)): Flow visualization image of dtype uint8.\n",
    "#     \"\"\"\n",
    "\n",
    "#     N, _, H, W = normalized_flow.shape\n",
    "#     device = normalized_flow.device\n",
    "#     flow_image = torch.zeros((N, 3, H, W), dtype=torch.uint8, device=DEVICE)\n",
    "#     colorwheel = _make_colorwheel().to(DEVICE)  # shape [55x3]\n",
    "#     num_cols = colorwheel.shape[0]\n",
    "#     norm = torch.sum(normalized_flow**2, dim=1).sqrt()\n",
    "#     a = (\n",
    "#         torch.atan2(-normalized_flow[:, 1, :, :], -normalized_flow[:, 0, :, :])\n",
    "#         / torch.pi\n",
    "#     )\n",
    "#     fk = (a + 1) / 2 * (num_cols - 1)\n",
    "#     k0 = torch.floor(fk).to(torch.long)\n",
    "#     k1 = k0 + 1\n",
    "#     k1[k1 == num_cols] = 0\n",
    "#     f = fk - k0\n",
    "\n",
    "#     for c in range(colorwheel.shape[1]):\n",
    "#         tmp = colorwheel[:, c]\n",
    "#         col0 = tmp[k0] / 255.0\n",
    "#         col1 = tmp[k1] / 255.0\n",
    "#         col = (1 - f) * col0 + f * col1\n",
    "#         col = 1 - norm * (1 - col)\n",
    "#         flow_image[:, c, :, :] = torch.floor(255 * col)\n",
    "#     return flow_image\n",
    "\n",
    "\n",
    "# def _make_colorwheel() -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Generates a color wheel for optical flow visualization as presented in:\n",
    "#     Baker et al. \"A Database and Evaluation Methodology for Optical Flow\" (ICCV, 2007)\n",
    "#     URL: http://vision.middlebury.edu/flow/flowEval-iccv07.pdf.\n",
    "\n",
    "#     Returns:\n",
    "#         colorwheel (Tensor[55, 3]): Colorwheel Tensor.\n",
    "#     \"\"\"\n",
    "\n",
    "#     RY = 15\n",
    "#     YG = 6\n",
    "#     GC = 4\n",
    "#     CB = 11\n",
    "#     BM = 13\n",
    "#     MR = 6\n",
    "\n",
    "#     ncols = RY + YG + GC + CB + BM + MR\n",
    "#     colorwheel = torch.zeros((ncols, 3))\n",
    "#     col = 0\n",
    "\n",
    "#     # RY\n",
    "#     colorwheel[0:RY, 0] = 255\n",
    "#     colorwheel[0:RY, 1] = torch.floor(255 * torch.arange(0, RY) / RY)\n",
    "#     col = col + RY\n",
    "#     # YG\n",
    "#     colorwheel[col : col + YG, 0] = 255 - torch.floor(255 * torch.arange(0, YG) / YG)\n",
    "#     colorwheel[col : col + YG, 1] = 255\n",
    "#     col = col + YG\n",
    "#     # GC\n",
    "#     colorwheel[col : col + GC, 1] = 255\n",
    "#     colorwheel[col : col + GC, 2] = torch.floor(255 * torch.arange(0, GC) / GC)\n",
    "#     col = col + GC\n",
    "#     # CB\n",
    "#     colorwheel[col : col + CB, 1] = 255 - torch.floor(255 * torch.arange(CB) / CB)\n",
    "#     colorwheel[col : col + CB, 2] = 255\n",
    "#     col = col + CB\n",
    "#     # BM\n",
    "#     colorwheel[col : col + BM, 2] = 255\n",
    "#     colorwheel[col : col + BM, 0] = torch.floor(255 * torch.arange(0, BM) / BM)\n",
    "#     col = col + BM\n",
    "#     # MR\n",
    "#     colorwheel[col : col + MR, 2] = 255 - torch.floor(255 * torch.arange(MR) / MR)\n",
    "#     colorwheel[col : col + MR, 0] = 255\n",
    "#     return colorwheel\n",
    "\n",
    "\n",
    "# def _generate_color_palette(num_objects: int):\n",
    "#     palette = torch.tensor([2**25 - 1, 2**15 - 1, 2**21 - 1])\n",
    "#     return [tuple((i * palette) % 255) for i in range(num_objects)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59d3a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating optical flow:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e40e2269aad4c90984850c17b840acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/448 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:/Users/vanag/Desktop/flow_imgs/ for conversion to video:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480c7a6f52cf4d1f8592c5511a9da68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting C:/Users/vanag/Desktop/flow_imgs/ to video:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79d93f127a14a8abc48f91e90688d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mariupol res 1280x720\n",
    "# pesmod res 1920x1080\n",
    "\n",
    "# UPDATE RESOLUTION IN PREPROCESS\n",
    "\n",
    "thresholds = np.array([1])\n",
    "\n",
    "THRESHOLD = 1\n",
    "HEIGHT = 720\n",
    "WIDTH = 1280\n",
    "ALGORITHM = \"none\"\n",
    "INPUT_PATH = \"C:/Users/vanag/Desktop/IMG_6111_1.MOV\"\n",
    "OUTPUT_PHOTOS_PATH = \"C:/Users/vanag/Desktop/flow_imgs/\"\n",
    "OUTPUT_VIDEO_PATH = INPUT_PATH.strip(\".mp4\") + \"_test\"\n",
    "os.makedirs(OUTPUT_PHOTOS_PATH, exist_ok=True)\n",
    "\n",
    "frames, list_of_flows, predicted_flows, flow_imgs, grid = [], [], [], [], []\n",
    "\n",
    "START_TIME = 45\n",
    "END_TIME = 60\n",
    "\n",
    "frames, _, _ = read_video(\n",
    "    INPUT_PATH, start_pts=START_TIME, end_pts=END_TIME, pts_unit=\"sec\"\n",
    ")\n",
    "frames = frames.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "num_frames = len(frames)\n",
    "FPS = num_frames / (END_TIME - START_TIME)\n",
    "indices = np.arange(0, num_frames)\n",
    "reduction_times = np.array([])\n",
    "full_flows_stack = torch.empty([(len(indices) - 2), 2, HEIGHT, WIDTH])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for THRESHOLD in thresholds:\n",
    "    print(\"Calculating optical flow:\")\n",
    "    for i in tqdm(indices[0:-2]):\n",
    "        img_1 = frames[i : (i + 1)]\n",
    "        img_2 = frames[(i + 1) : (i + 2)]\n",
    "        img_1_preprocessed = preprocess(img_1).to(DEVICE)\n",
    "        img_2_preprocessed = preprocess(img_2).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            list_of_flows = model(\n",
    "                img_1_preprocessed.to(DEVICE), img_2_preprocessed.to(DEVICE)\n",
    "            )\n",
    "        flow = list_of_flows[-1][0]\n",
    "        full_flows_stack[i] = flow\n",
    "        t_0 = time.time()\n",
    "        if ALGORITHM == \"cosine\":\n",
    "            flow = cosine_distance_background_reduction(flow, threshold=float(THRESHOLD))\n",
    "        elif ALGORITHM == \"plane\":\n",
    "            flow = get_detection(flow, threshold=float(THRESHOLD))\n",
    "        elif ALGORITHM != \"none\":\n",
    "            break\n",
    "        t_1 = time.time()\n",
    "        reduction_times = np.append(reduction_times, t_1 - t_0)\n",
    "        flow_img = flow_to_image(flow)\n",
    "        (F.to_pil_image(flow_img)).save(OUTPUT_PHOTOS_PATH + str(i) + \".jpg\")\n",
    "#         cv2_img = cv2.cvtColor(np.asarray(F.to_pil_image(flow_img)) , cv2.COLOR_BGR2GRAY)\n",
    "#         ret, otsu = cv2.threshold(cv2_img, 0, 255, cv2.THRESH_OTSU)\n",
    "#         cv2.imwrite(OUTPUT_PHOTOS_PATH + str(i) + \".jpg\", otsu)\n",
    "        \n",
    "    \n",
    "    OUTPUT_VIDEO_PATH = (\n",
    "        OUTPUT_VIDEO_PATH\n",
    "        + \"_\"\n",
    "        +str(START_TIME)\n",
    "        +\"-\"\n",
    "        +str(END_TIME)\n",
    "        +\"-\"\n",
    "        + ALGORITHM\n",
    "        + \"_\"\n",
    "        + str(THRESHOLD)\n",
    "        + \"_\"\n",
    "        + str(np.mean(reduction_times)).replace(\".\", \",\")\n",
    "        + \".mp4\"\n",
    "    )\n",
    "    torch.save(full_flows_stack, OUTPUT_VIDEO_PATH.replace(\".mp4\",\".pt\"))\n",
    "    concatenate_frames2video(\n",
    "        OUTPUT_PHOTOS_PATH, INPUT_PATH, OUTPUT_VIDEO_PATH, START_TIME, END_TIME, FPS\n",
    "    )\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef276c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
