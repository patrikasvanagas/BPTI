{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe05c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "from torchvision.utils import flow_to_image\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.io import read_video\n",
    "from pytube import YouTube\n",
    "from tqdm.notebook import tqdm\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "import ffmpeg\n",
    "import winsound\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = raft_large(pretrained=True, progress=False).to(DEVICE)\n",
    "model = model.eval()\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d18680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(imgs, figsize=None, **imshow_kwargs):\n",
    "    if not isinstance(imgs[0], list):\n",
    "        # Make a 2d grid even if there's just 1 row\n",
    "        imgs = [imgs]\n",
    "    num_rows = len(imgs)\n",
    "    num_cols = len(imgs[0])\n",
    "    _, axs = plt.subplots(\n",
    "        nrows=num_rows, ncols=num_cols, squeeze=False, figsize=figsize\n",
    "    )\n",
    "    for row_idx, row in enumerate(imgs):\n",
    "        for col_idx, img in enumerate(row):\n",
    "            ax = axs[row_idx, col_idx]\n",
    "            img = F.to_pil_image(img.to(\"cpu\"))\n",
    "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
    "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def preprocess(batch):\n",
    "    # image dimension must be divisible by 8\n",
    "    transforms = T.Compose(\n",
    "        [\n",
    "            T.ConvertImageDtype(torch.float32),\n",
    "            T.Normalize(mean=0.5, std=0.5),  # map [0, 1] into [-1, 1]\n",
    "            T.Resize(size=(HEIGHT, WIDTH)),\n",
    "        ]\n",
    "    )\n",
    "    batch = transforms(batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "# def download_youtube(videourl, path):\n",
    "#     yt = YouTube(videourl)\n",
    "#     # title = yt.streams[0].title\n",
    "#     yt = (\n",
    "#         yt.streams.filter(progressive=True, file_extension=\"mp4\")\n",
    "#         .order_by(\"resolution\")\n",
    "#         .desc()\n",
    "#         .first()\n",
    "#     )\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "#     yt.download(path)\n",
    "#     # return title\n",
    "\n",
    "\n",
    "def sorted_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", key)]\n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "\n",
    "def vid2frame(path_in):\n",
    "    old_wd = os.getcwd()\n",
    "    vidcap = cv2.VideoCapture(path_in)\n",
    "    # success, image = vidcap.read()\n",
    "    image = vidcap.read()[1]\n",
    "    frames = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    path_out = path_in.replace(\".mp4\", \"_frames/\")\n",
    "    os.makedirs(path_out, exist_ok=True)\n",
    "    for o in os.listdir(path_out):\n",
    "        os.remove(os.path.join(path_out, o))\n",
    "    os.chdir(path_out)\n",
    "    print(\"Converting \" + path_in + \" to frames:\")\n",
    "    for m in tqdm(range(frames)):\n",
    "        cv2.imwrite(\"%d.jpg\" % m, image)\n",
    "        # success, image = vidcap.read()\n",
    "        image = vidcap.read()[1]\n",
    "    vidcap.release()\n",
    "    os.chdir(old_wd)\n",
    "\n",
    "\n",
    "def frame2vid(path_in, path_out, fps):\n",
    "    old_wd = os.getcwd()\n",
    "    # path_out = path_in.strip(\"/\") + \".mp4\"\n",
    "    frame_array = []\n",
    "    files = [f for f in os.listdir(path_in) if os.path.isfile(os.path.join(path_in, f))]\n",
    "    files = sorted_alphanumeric(files)\n",
    "    print(\"Reading \" + path_in + \" for conversion to video:\")\n",
    "    for m in tqdm(range(len(files))):\n",
    "        filename = path_in + files[m]\n",
    "        img = cv2.imread(filename)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width, height)\n",
    "        frame_array.append(img)\n",
    "    out = cv2.VideoWriter(path_out, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, size)\n",
    "    print(\"Converting \" + path_in + \" to video:\")\n",
    "    for m in tqdm(range(len(frame_array))):\n",
    "        out.write(frame_array[m])\n",
    "    out.release()\n",
    "    os.chdir(old_wd)\n",
    "\n",
    "\n",
    "def concatenate_frames2video(\n",
    "    path_in_of, path_video, path_out, start_time, end_time, fps\n",
    "):\n",
    "    old_wd = os.getcwd()\n",
    "    frame_array = []\n",
    "    files = [\n",
    "        f for f in os.listdir(path_in_of) if os.path.isfile(os.path.join(path_in_of, f))\n",
    "    ]\n",
    "    files = sorted_alphanumeric(files)\n",
    "    original_frames, _, _ = torchvision.io.read_video(\n",
    "        path_video, start_pts=start_time, end_pts=end_time, pts_unit=\"sec\"\n",
    "    )\n",
    "    original_frames = original_frames.permute(\n",
    "        0, 3, 1, 2\n",
    "    )  # (N, H, W, C) -> (N, C, H, W)\n",
    "    num_frames = len(original_frames)\n",
    "    indices = np.arange(0, num_frames)\n",
    "    print(\"Reading \" + path_in_of + \" for conversion to video:\")\n",
    "    for m in tqdm(range(len(files))):\n",
    "        filename = path_in_of + files[m]\n",
    "        right_img = cv2.imread(filename)\n",
    "        left_img = cv2.resize(\n",
    "            np.array(F.to_pil_image(original_frames[m - 1]))[:, :, ::-1],\n",
    "            (WIDTH, HEIGHT),\n",
    "        )\n",
    "        img = np.concatenate([left_img, right_img], axis=1)\n",
    "        height, width, layers = img.shape\n",
    "        size = (width, height)\n",
    "        frame_array.append(img)\n",
    "    out = cv2.VideoWriter(path_out, cv2.VideoWriter_fourcc(*\"MP4V\"), fps, size)\n",
    "    print(\"Converting \" + path_in_of + \" to video:\")\n",
    "    for m in tqdm(range(len(frame_array))):\n",
    "        out.write(frame_array[m])\n",
    "    out.release()\n",
    "    os.chdir(old_wd)\n",
    "\n",
    "\n",
    "def cosine_distance_background_reduction(input_flow, threshold=1):\n",
    "    input_flow_mean = torch.stack(\n",
    "        (\n",
    "            torch.ones(input_flow[0].size(), device=DEVICE) * input_flow[0].mean(),\n",
    "            torch.ones(input_flow[0].size(), device=DEVICE) * input_flow[1].mean(),\n",
    "        )\n",
    "    )\n",
    "    calculate_cosine_similarity = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    cosine_similarity = calculate_cosine_similarity(input_flow, input_flow_mean)\n",
    "    mean_cosine_similarity = cosine_similarity.mean()\n",
    "    indices_to_keep = cosine_similarity < mean_cosine_similarity * threshold\n",
    "    output_flow = input_flow * indices_to_keep\n",
    "    return output_flow\n",
    "\n",
    "def get_grid(shape):\n",
    "    x = torch.arange(1, shape[0] + 1, 1.0)\n",
    "    y = torch.arange(1, shape[1] + 1, 1.0)\n",
    "\n",
    "    return torch.meshgrid(x, y, indexing=\"ij\")\n",
    "\n",
    "# Operates on GPU\n",
    "def fit_2d_surface(x_grid, y_grid, z):\n",
    "    features = torch.stack([\n",
    "        torch.ones_like(x_grid), x_grid, y_grid, x_grid * x_grid, x_grid * y_grid, y_grid * y_grid\n",
    "    ]).reshape((6, -1)).T.cuda()\n",
    "\n",
    "    solution = torch.linalg.lstsq(features, z.flatten())\n",
    "\n",
    "    z_fitted = (features @ solution.solution).reshape(z.shape)\n",
    "\n",
    "    z_residuals = z - z_fitted\n",
    "\n",
    "    return solution, z_fitted, z_residuals\n",
    "\n",
    "# Flow must be cuda of shape (2, H, W)\n",
    "def get_detection_mask(flow, threshold):\n",
    "    flow_magnitudes = torch.linalg.norm(flow, axis=0)\n",
    "    x_grid, y_grid = get_grid(shape=flow_magnitudes.shape)\n",
    "\n",
    "    solution, flow_magnitudes_fitted, flow_magnitude_residuals = fit_2d_surface(\n",
    "        x_grid, y_grid, flow_magnitudes\n",
    "    )\n",
    "\n",
    "    return torch.abs(flow_magnitude_residuals) > threshold\n",
    "\n",
    "def get_detection(flow, threshold=1.25):\n",
    "    detection_mask = get_detection_mask(flow, threshold=threshold)\n",
    "\n",
    "    return detection_mask * flow\n",
    "\n",
    "def ffmpeg_xstack(input_path, output_path, *, fps):\n",
    "    inputs = [\n",
    "\n",
    "        ffmpeg.input(subdir / '%d.jpg', framerate=fps)\n",
    "        for subdir in sorted(input_path.iterdir())\n",
    "        if subdir.is_dir()\n",
    "    ]\n",
    "\n",
    "    ffmpeg \\\n",
    "        .filter(inputs, 'xstack', inputs=4, layout='0_0|w0_0|0_h0|w0_h0') \\\n",
    "        .output(str(output_path)) \\\n",
    "        .run(overwrite_output=True)\n",
    "\n",
    "    return Video(output_path, width=1280, height=720)\n",
    "\n",
    "\n",
    "def ffmpeg_hstack(video_path, frames_path, output_path, *, fps):\n",
    "    inputs = [\n",
    "        ffmpeg.input(video_path, ss=FROM, to=TO),\n",
    "        ffmpeg.input(frames_path / '%d.jpg', framerate=fps)\n",
    "    ]\n",
    "\n",
    "    ffmpeg \\\n",
    "        .filter(inputs, 'hstack') \\\n",
    "        .output(str(output_path)) \\\n",
    "        .run(overwrite_output=True)\n",
    "\n",
    "    return Video(output_path, width=1280, height=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.array([1])\n",
    "\n",
    "THRESHOLD = 1\n",
    "HEIGHT = 720\n",
    "WIDTH = 1280\n",
    "ALGORITHM = \"cosine\"\n",
    "INPUT_PATH = \"C:/Users/pv5/Desktop/Clock_Face_3Videvo_preview.mp4\"\n",
    "OUTPUT_PHOTOS_PATH = \"C:/pv5/pv5/Desktop/bike1_flow/\"\n",
    "OUTPUT_VIDEO_PATH = INPUT_PATH.strip(\".mp4\") + \"_flow\"\n",
    "os.makedirs(OUTPUT_PHOTOS_PATH, exist_ok=True)\n",
    "\n",
    "frames, list_of_flows, predicted_flows, flow_imgs, grid = [], [], [], [], []\n",
    "\n",
    "START_TIME = 0\n",
    "END_TIME = 7 #add 1 extra second\n",
    "\n",
    "frames, _, _ = read_video(\n",
    "    INPUT_PATH, start_pts=START_TIME, end_pts=END_TIME, pts_unit=\"sec\"\n",
    ")\n",
    "frames = frames.permute(0, 3, 1, 2)  # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "num_frames = len(frames)\n",
    "FPS = num_frames / (END_TIME - START_TIME)\n",
    "indices = np.arange(0, num_frames)\n",
    "reduction_times = np.array([])\n",
    "full_flows_stack = torch.empty([(len(indices) - 2), 2, HEIGHT, WIDTH])\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for THRESHOLD in thresholds:\n",
    "    print(\"Calculating optical flow:\")\n",
    "    for i in tqdm(indices[0:-2]):\n",
    "            img_1 = frames[i : (i + 1)]\n",
    "            img_2 = frames[(i + 1) : (i + 2)]\n",
    "            img_1_preprocessed = preprocess(img_1).to(DEVICE)\n",
    "            img_2_preprocessed = preprocess(img_2).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                list_of_flows = model(\n",
    "                    img_1_preprocessed.to(DEVICE), img_2_preprocessed.to(DEVICE)\n",
    "                )\n",
    "            flow = list_of_flows[-1][0]\n",
    "            full_flows_stack[i] = flow\n",
    "            t_0 = time.time()\n",
    "            if ALGORITHM == \"cosine\":\n",
    "                flow = cosine_distance_background_reduction(flow, threshold=float(THRESHOLD))\n",
    "            elif ALGORITHM == \"plane\":\n",
    "                flow = get_detection(flow, threshold=float(THRESHOLD))\n",
    "            elif ALGORITHM != \"none\":\n",
    "                break\n",
    "            t_1 = time.time()\n",
    "            reduction_times = np.append(reduction_times, t_1 - t_0)\n",
    "            flow_img = flow_to_image(flow)\n",
    "            (F.to_pil_image(flow_img)).save(OUTPUT_PHOTOS_PATH + str(i) + \".jpg\")\n",
    "        \n",
    "    \n",
    "    OUTPUT_VIDEO_PATH = (\n",
    "        OUTPUT_VIDEO_PATH\n",
    "        + \"_\"\n",
    "        +str(START_TIME)\n",
    "        +\"-\"\n",
    "        +str(END_TIME)\n",
    "        +\"-\"\n",
    "        + ALGORITHM\n",
    "        + \"_\"\n",
    "        + str(THRESHOLD)\n",
    "        + \"_\"\n",
    "        + str(np.mean(reduction_times)).replace(\".\", \",\")\n",
    "        + \".mp4\"\n",
    "    )\n",
    "    torch.save(full_flows_stack, OUTPUT_VIDEO_PATH.replace(\".mp4\",\".pt\"))\n",
    "    concatenate_frames2video(\n",
    "        OUTPUT_PHOTOS_PATH, INPUT_PATH, OUTPUT_VIDEO_PATH, START_TIME, END_TIME, FPS\n",
    "    )\n",
    "\n",
    "    frame2vid(OUTPUT_PHOTOS_PATH, OUTPUT_VIDEO_PATH, FPS)\n",
    "    \n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 440  # Hz\n",
    "winsound.Beep(freq, duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
